{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5f640e7",
   "metadata": {},
   "source": [
    "## This is the script to work with the quora dataset\n",
    "The code is a copy of https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "\n",
    "* Dataset consists of over 400,000 lines of potential question duplicate pairs.\n",
    "* Each line contains IDs for each question in the pair, the full text for each question, and a binary value that indicates whether the line truly contains a duplicate pair.\n",
    "* Data Fields:\n",
    "\n",
    "    id - the id of a training set question pair\n",
    "\n",
    "    qid1, qid2 - unique ids of each question (only available in train.csv)\n",
    "\n",
    "    question1, question2 - the full text of each question\n",
    "\n",
    "    is_duplicate - the target variable, set to 1 if question1 and question2 have essentially the same meaning, and 0 otherwise.\n",
    "\n",
    "The sample of the data is as follows\n",
    "\n",
    "![image](https://qph.fs.quoracdn.net/main-qimg-ea50c7a005eb7750af0b53b07c8caa60)\n",
    "\n",
    "\n",
    "\n",
    "The goal is to use this data and formulate it into a __seq2seq problem__. In this regards we have formulated the following problem statement\n",
    "\n",
    "**Problem Statement**\n",
    "\n",
    "Given the Quora dataset, take out the questions pairs where question1 is a duplicate of question2 basically where __is_duplicate=1__. \n",
    "\n",
    "Now the seq2seq problem can be formulated as given question1 to the encoder, decoder should be able to find out the corresponding question2. (This is similar to saying given an english statement find the equivalent french statement)\n",
    "\n",
    "\n",
    "**Approach**\n",
    "\n",
    "1. Convert the quora dataset to a seq2seq network compatible dataset\n",
    "2. Read the data into the network \n",
    "3. Train the network using the example code\n",
    "4. Evaluate the training accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e45df21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289e7cde",
   "metadata": {},
   "source": [
    "## Create the dataset\n",
    "1. read the quora original data set and convert it to the q1,q2 pairs\n",
    "2. in order to reuse the data preparation code we need to save it to a tab separated file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "020186c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make the dataset \n",
    "quoraDF=pd.read_csv(\"data/quora_raw_train.csv\")\n",
    "quoraDF=quoraDF[quoraDF['is_duplicate']==1][['question1','question2']]\n",
    "quoraDF.head()\n",
    "## save it to the file\n",
    "quoraDF.to_csv(\"data/quora_similar_questions.txt\",sep=\"\\t\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a11ddd9",
   "metadata": {},
   "source": [
    "### Dataset preparation code as copy pasted. \n",
    "Lang1, Lang2 refer to question1 and question2\n",
    "\n",
    "**minor changes**\n",
    "1. we are not filering out on the basis of the eng_prefixes\n",
    "2. also the file name has been hardcoded instead of picking as the languge names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3de9fb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e0fa567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "463b82fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/quora_similar_questions.txt', encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f475be77",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e639278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 149264 sentence pairs\n",
      "Trimmed to 44289 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "quest1 13298\n",
      "quest2 13275\n",
      "['which is the strongest character in mahabharata ?', 'who was the strongest character in mahabharata ?']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('quest1', 'quest2')\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf694da",
   "metadata": {},
   "source": [
    "## Encoder class \n",
    "It follows are basic architecture of Input Tensor >> Emebedding Layer >> GRU\n",
    "\n",
    "Refer to the diagram \n",
    "![image](img/encoder.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e223a832",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6b9e47",
   "metadata": {},
   "source": [
    "## Attention Mechanism\n",
    "\n",
    "This is the main class that defines the attention mechanism along with the decoder part\n",
    "\n",
    "The attention has been calculated little differently than the usual method (Additive Attention). \n",
    "\n",
    "I will explain both the ways\n",
    "\n",
    "1. Attention architecture as used in the code:\n",
    "![image](img/attention_architecture_example.png)\n",
    "\n",
    "\n",
    "\n",
    "Following is the approach\n",
    "\n",
    "1. Calculate the attention weights which is set of $\\alpha$s. We make use of $D_t\\__1$ which is the previous hidden state and the input at time t which is same as Output of the previous state. we denote it as $O_t\\__1$ \n",
    "so Attention weight is a function of  $D_t\\__1$ & $O_t\\__1$ \n",
    "\n",
    "2. Now $D_t\\__1$ & $O_t\\__1$  is passed to a FC to get a set of paramters that can be trained in order to find the real values of set of $\\alpha$ => $\\{Î±_1,\\alpha_2....\\alpha_N\\} $\n",
    "\n",
    "![image](img/attention_weights1.png)\n",
    "\n",
    "\n",
    "3. The attention weights are then applied to the Encoded_Outputs which is essentially all the hidden states of the Encoder so that we can now choose which all Encoder outputs are going to affect the Decoder most. The attention is essentially a set of weights to pick up the right percentage of the Encoder_outputs\n",
    "\n",
    "  $\\alpha . E_o $.. this is esentially equal to $\\alpha_1 * E_1 + \\alpha_2*E_2 +....\\alpha_N* E_N $\n",
    "\n",
    "  the above step is also called the __attention applied__ and is performed by a special function called __BMM__.  $\\alpha . E_o $ is also called the __Context Vector__\n",
    "\n",
    "4. Now we have the weighted effect of Encoder outputs ready . So we combine it with the input of the decoder which is $O_t\\__1$\n",
    "So basically $[O_t\\__1,\\alpha.E_o]$\n",
    "This step is also called the __attention_combined__\n",
    "\n",
    "5. The __attention_combined__ is sent to the GRU and the output & hidded i.e. $O_t$ & $D_t$ for the state t are obtained\n",
    "\n",
    "Now the __biggest concern__ is the attention step which is step2. Why are hidden state combined with the input. They will be almost the same . The idea is to check the effect of Encoder states on the decoder states and so attention should be calculated using Encoder outputs rather than the Decoder Output\n",
    "\n",
    "\n",
    "\n",
    "### Alternate approach. (additive Attention)\n",
    "\n",
    "The main difference is the paramters that are used to create the attention weights\n",
    "\n",
    "The attention is calculated using the $D_t\\__1$ and Encoder_Outputs ($E_o$)\n",
    "\n",
    "So the attention block will look like the following\n",
    "![image](img/attention_weights2.png)\n",
    "\n",
    "\n",
    "And then the Set of alphas are combined with the Encoder_outputs to get the __Context Vector__ or what was called earlier as __attention_applied__ done using __bmm__\n",
    "\n",
    "Other steps remains more or less similar\n",
    "\n",
    "The major advantage is that the alphas are learnt after the interaction between the decoder and the Encoder which is what we wanted that decoder should find out how much different Encoder Outputs are required for the new prediction\n",
    "The following is the complete architecture\n",
    "\n",
    "![image](img/attention_calculation_Additive.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd3adddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this class is implementing the additive attention given by\n",
    "## attn_weights=softmax(W_combined*tanh(W_decoder*decoder_hidden+W_Encoder*encoder_values))\n",
    "class AdditiveAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_dim, decoder_dim):\n",
    "        super(AdditiveAttention,self).__init__()\n",
    "        #256 this is the dimention of the D(t-1)\n",
    "        self.decoder_dim=decoder_dim \n",
    "        ## the diemtion of each outut of the encoder...this in our case is 256 \n",
    "        self.encoder_dim=encoder_dim\n",
    "        ## this is one of the weight matrix that will be sed to combine the weitghs\n",
    "        self.W_Combined = torch.nn.Parameter(\n",
    "            torch.FloatTensor(self.decoder_dim).uniform_(-0.1, 0.1)) \n",
    "        #FC to add a weight matrix W_decoder for the Decoder_Hidden\n",
    "        self.W_Decoder = torch.nn.Linear(decoder_dim, decoder_dim)\n",
    "        #FC to add a weight matrix to the encoder_values\n",
    "        self.W_EncoderOutput = torch.nn.Linear(encoder_dim, decoder_dim)\n",
    "\n",
    "    ##query and values is a generic terms used in case of attention \n",
    "    def get_weights(self,        \n",
    "        query: torch.Tensor,  # query = decoder hidden\n",
    "        values: torch.Tensor,  # encoder output of the form maxLength*encoder_dim in ou r case it is 10x256\n",
    "    ):\n",
    "        attn_weights = self.W_Decoder(query) + self.W_EncoderOutput(values)  # [max_length, decoder_dim]\n",
    "        return F.softmax((torch.tanh(attn_weights) @ self.W_Combined),dim=1) # [1,max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e23eb6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is another way of implementing the attention. In this case the attention is given by \n",
    "## attn_weight=softmax(decoder_hidden*W*encoder_values)\n",
    "\n",
    "class MultiplicativeAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_dim: int, decoder_dim: int):\n",
    "        super(MultiplicativeAttention,self).__init__()\n",
    "        ##256\n",
    "        self.decoder_dim=decoder_dim\n",
    "        ##dimention of each encoder output 256\n",
    "        self.encoder_dim=encoder_dim\n",
    "        ## weight vector of size 256\n",
    "        self.W = torch.nn.Parameter(torch.FloatTensor(\n",
    "            self.decoder_dim, self.encoder_dim).uniform_(-0.1, 0.1))\n",
    "\n",
    "    def get_weights(self,\n",
    "        query: torch.Tensor,  # [decoder_dim]\n",
    "        values: torch.Tensor, # [seq_length, encoder_dim]\n",
    "    ):\n",
    "        weights = query @ self.W @ values.T  # [1,max_length]\n",
    "        return F.softmax((weights/np.sqrt(self.decoder_dim)),dim=1)  # [1,max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ace198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is what has been used in the code https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "## basically the attention is given by  softmax(W*(Decoder_input_attime_t + Decoder_hidden_attime_t-1))\n",
    "class PytorchAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, max_length, decoder_dim):\n",
    "        super(PytorchAttention,self).__init__()\n",
    "        #256 this is the dimention of the D(t-1)\n",
    "        self.decoder_dim=decoder_dim \n",
    "        # the  maximul legth of the sequence..in our case it is 10\n",
    "        self.max_length=max_length \n",
    "        ## this will add a Weight\n",
    "        self.attn = nn.Linear(self.decoder_dim * 2, self.max_length)\n",
    "       \n",
    "    def get_weights(self,        \n",
    "        \n",
    "        hidden: torch.Tensor,  #decoder hidden at time t-1                  \n",
    "        embedded: torch.Tensor #decoder input at time t\n",
    "    ):\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded, hidden), 1)), dim=1)  # [1,seq_length]\n",
    "        return attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0440958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, attentionType=\"Pytorch\",dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "#         self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "         ## this is different from the original code\n",
    "        self.attentionType=attentionType\n",
    "        if(self.attentionType==\"Multiplicative\"):\n",
    "            self.attention = MultiplicativeAttention(self.hidden_size,self.hidden_size)\n",
    "        elif(self.attentionType==\"Additive\"):\n",
    "            self.attention = AdditiveAttention(self.hidden_size,self.hidden_size)\n",
    "        elif(self.attentionType==\"Pytorch\"):\n",
    "            self.attention = PytorchAttention(self.max_length,self.hidden_size)\n",
    "       \n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        \n",
    "        if(self.attentionType==\"Multiplicative\"):\n",
    "            attn_weights=self.attention.get_weights(hidden[0],encoder_outputs)\n",
    "        elif(self.attentionType==\"Additive\"):\n",
    "            attn_weights=self.attention.get_weights(hidden[0],encoder_outputs.unsqueeze(0))\n",
    "        elif(self.attentionType==\"Pytorch\"):\n",
    "            attn_weights = self.attention.get_weights(hidden[0],embedded[0])\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "#         attn_weights = F.softmax(\n",
    "#             self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "509eba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0144e57",
   "metadata": {},
   "source": [
    "#### we are making use of the Teacher Forcing which is a way of sending the actual ground truth while the trainign is going on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93779c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c73d73",
   "metadata": {},
   "source": [
    "#### utility function for capturing the time of  execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "28e8ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c607a3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "309480e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf08d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84c9bb5",
   "metadata": {},
   "source": [
    "### function to fetch the random samples and evaluate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a323098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b0bb23",
   "metadata": {},
   "source": [
    "### the actual training starts here with 75000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "70884b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 22s (- 19m 20s) (5000 6%) 4.3533\n",
      "2m 42s (- 17m 37s) (10000 13%) 3.8265\n",
      "4m 3s (- 16m 13s) (15000 20%) 3.5220\n",
      "5m 23s (- 14m 49s) (20000 26%) 3.3943\n",
      "6m 44s (- 13m 28s) (25000 33%) 3.2572\n",
      "8m 4s (- 12m 7s) (30000 40%) 3.1121\n",
      "9m 25s (- 10m 46s) (35000 46%) 3.0206\n",
      "10m 46s (- 9m 25s) (40000 53%) 2.9435\n",
      "12m 6s (- 8m 4s) (45000 60%) 2.8835\n",
      "13m 27s (- 6m 43s) (50000 66%) 2.8100\n",
      "14m 47s (- 5m 22s) (55000 73%) 2.7668\n",
      "16m 8s (- 4m 2s) (60000 80%) 2.7245\n",
      "17m 29s (- 2m 41s) (65000 86%) 2.6653\n",
      "18m 50s (- 1m 20s) (70000 93%) 2.5953\n",
      "20m 10s (- 0m 0s) (75000 100%) 2.5659\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words,attentionType=\"Pytorch\", dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "864e3aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> why do many muslims hate rss ?\n",
      "= why most of the muslims hate rss ?\n",
      "< why do india hate ? <EOS>\n",
      "\n",
      "> why does nobody like nickelback ?\n",
      "= why do so many people dislike nickelback ?\n",
      "< why do some people like dogs ? <EOS>\n",
      "\n",
      "> how can i overcome sadness after a breakup ?\n",
      "= how do i overcome depression after breakup ?\n",
      "< how can i overcome to to ? ? <EOS>\n",
      "\n",
      "> what is the latest computer programming language ?\n",
      "= what are the latest programming languages ?\n",
      "< what is the programming language language ? ? <EOS>\n",
      "\n",
      "> how can i use quora to make money ?\n",
      "= do bloggers on quora earn money ?\n",
      "< how do i ways money from ? ? <EOS>\n",
      "\n",
      "> why is saltwater taffy candy imported in germany ?\n",
      "= why is saltwater taffy candy imported in brazil ?\n",
      "< why is saltwater taffy candy imported in ? <EOS>\n",
      "\n",
      "> how can i understand english ?\n",
      "= how can i become good at english ?\n",
      "< how can i learn english english ? <EOS>\n",
      "\n",
      "> how did donald trump win the elections ?\n",
      "= how did donald trump win ?\n",
      "< how did trump trump the the election ? <EOS>\n",
      "\n",
      "> what is politics in islam ?\n",
      "= what is political islam ?\n",
      "< what is islam ? <EOS>\n",
      "\n",
      "> what constitutes true beauty ?\n",
      "= what is true beauty ?\n",
      "< what are the beauty ? <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## check the results \n",
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234cf22f",
   "metadata": {},
   "source": [
    "### code to visually see the attention while performing the translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4e51f83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dc0b5355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = what is are your new year resolutions for\n",
      "output = what s your new year resolutions for ? <EOS>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauravp/python36/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/gauravp/python36/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluateAndShowAttention(\"what is are your new year resolutions for\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ee737",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
