{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "576fcb7b",
   "metadata": {},
   "source": [
    "## This is the script to work with the question answers pair \n",
    "The code is a copy of https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "\n",
    "We have used the dataset which is present at https://www.kaggle.com/c/quora-question-pairs/data .. this is the same data as in http://www.cs.cmu.edu/~ark/QA-data/\n",
    " \n",
    "The ideas is to traina netwrok that can give out the answer given a question. This is a seq2seq kind of a problem where Encoder will use Questions as the input and Output of the Decoders is corresponding Answer\n",
    "\n",
    "The dataset is distributed in 3 tab seperated files. The file entry is of the form\n",
    "\n",
    "ArticleTitle | Question | Answer | DifficultyFromQuestioner | DifficultyFromAnswerer | ArticleFile\n",
    "\n",
    "\n",
    "out of these 6 field we are interested in only Question & Answer so we will create a dataset containing just those 2 fields.\n",
    "\n",
    "There are 3423 samples to work with.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e45df21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c694e6ad",
   "metadata": {},
   "source": [
    "## Create the dataset\n",
    "1. read the data from the 3 files and then create  consolidated file\n",
    "2. Now select only the 2 fields that is Question, Answer and save then to a new tab separated file. This is done so that we can reuse the code as it is\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "85999761",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_08 = pd.read_csv('data/question_answer_pairs_1.txt', sep='\\t')\n",
    "df_09 = pd.read_csv('data/question_answer_pairs_2.txt', sep='\\t')\n",
    "df_10 = pd.read_csv('data/question_answer_pairs_3.txt', sep='\\t', encoding = 'ISO-8859-1')\n",
    "df_all = df_08.append([df_09, df_10])\n",
    "df_all_sub=df_all[['Question','Answer']]\n",
    "df_all_sub=df_all_sub.dropna()\n",
    "df_all_sub.to_csv(\"data/question_answer_pairs.txt\",sep=\"\\t\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aef729",
   "metadata": {},
   "source": [
    "### Dataset preparation code as copy pasted.\n",
    "Lang1, Lang2 refer to question and answer\n",
    "\n",
    "#### minor changes\n",
    "\n",
    "1. we are not filering out on the basis of the eng_prefixes\n",
    "2. also the file name has been hardcoded instead of picking as the languge names\n",
    "3. we are not removing the digits as some of the questions answer is the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3de9fb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3e0fa567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "##since some of the questions in this dataset have years as the answer we cannot remoe the digits\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    #s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "463b82fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/question_answer_pairs.txt', encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f475be77",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "# eng_prefixes = (\n",
    "#     \"i am \", \"i m \",\n",
    "#     \"he is\", \"he s \",\n",
    "#     \"she is\", \"she s \",\n",
    "#     \"you are\", \"you re \",\n",
    "#     \"we are\", \"we re \",\n",
    "#     \"they are\", \"they re \"\n",
    "# )\n",
    "\n",
    "## need to remove the eng_prefixes filter as this data set doesnot start with it\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0e639278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 3423 sentence pairs\n",
      "Trimmed to 1771 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "ques 1928\n",
      "ans 1594\n",
      "['did wilson support the committee system ?', 'no .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "## we want the processing to happen as given a question get the answer. If reverse is True then the traingin will be \n",
    "## given the answer what could be the question. \n",
    "input_lang, output_lang, pairs = prepareData('ques', 'ans', False)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488c5a04",
   "metadata": {},
   "source": [
    "## Encoder Class\n",
    "It follows are basic architecture of Input Tensor >> Emebedding Layer >> GRU\n",
    "\n",
    "Refer to the diagram \n",
    "![image](img/encoder.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e223a832",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc58881",
   "metadata": {},
   "source": [
    "## Attention Mechanism\n",
    "\n",
    "This is the main class that defines the attention mechanism along with the decoder part\n",
    "\n",
    "The attention has been calculated little differently than the usual method (Additive Attention). \n",
    "\n",
    "I will explain both the ways\n",
    "\n",
    "1. Attention architecture as used in the code:\n",
    "![image](img/attention_architecture_example.png)\n",
    "\n",
    "\n",
    "\n",
    "Following is the approach\n",
    "\n",
    "1. Calculate the attention weights which is set of $\\alpha$s. We make use of $D_t\\__1$ which is the previous hidden state and the input at time t which is same as Output of the previous state. we denote it as $O_t\\__1$ \n",
    "so Attention weight is a function of  $D_t\\__1$ & $O_t\\__1$ \n",
    "\n",
    "2. Now $D_t\\__1$ & $O_t\\__1$  is passed to a FC to get a set of paramters that can be trained in order to find the real values of set of $\\alpha$ => $\\{α_1,\\alpha_2....\\alpha_N\\} $\n",
    "\n",
    "![image](img/attention_weights1.png)\n",
    "\n",
    "\n",
    "3. The attention weights are then applied to the Encoded_Outputs which is essentially all the hidden states of the Encoder so that we can now choose which all Encoder outputs are going to affect the Decoder most. The attention is essentially a set of weights to pick up the right percentage of the Encoder_outputs\n",
    "\n",
    "  $\\alpha . E_o $.. this is esentially equal to $\\alpha_1 * E_1 + \\alpha_2*E_2 +....\\alpha_N* E_N $\n",
    "\n",
    "  the above step is also called the __attention applied__ and is performed by a special function called __BMM__.  $\\alpha . E_o $ is also called the __Context Vector__\n",
    "\n",
    "4. Now we have the weighted effect of Encoder outputs ready . So we combine it with the input of the decoder which is $O_t\\__1$\n",
    "So basically $[O_t\\__1,\\alpha.E_o]$\n",
    "This step is also called the __attention_combined__\n",
    "\n",
    "5. The __attention_combined__ is sent to the GRU and the output & hidded i.e. $O_t$ & $D_t$ for the state t are obtained\n",
    "\n",
    "Now the __biggest concern__ is the attention step which is step2. Why are hidden state combined with the input. They will be almost the same . The idea is to check the effect of Encoder states on the decoder states and so attention should be calculated using Encoder outputs rather than the Decoder Output\n",
    "\n",
    "\n",
    "\n",
    "### Alternate approach. (additive Attention)\n",
    "\n",
    "The main difference is the paramters that are used to create the attention weights\n",
    "\n",
    "The attention is calculated using the $D_t\\__1$ and Encoder_Outputs ($E_o$)\n",
    "\n",
    "So the attention block will look like the following\n",
    "![image](img/attention_weights2.png)\n",
    "\n",
    "\n",
    "And then the Set of alphas are combined with the Encoder_outputs to get the __Context Vector__ or what was called earlier as __attention_applied__ done using __bmm__\n",
    "\n",
    "Other steps remains more or less similar\n",
    "\n",
    "The major advantage is that the alphas are learnt after the interaction between the decoder and the Encoder which is what we wanted that decoder should find out how much different Encoder Outputs are required for the new prediction\n",
    "The following is the complete architecture\n",
    "\n",
    "![image](img/attention_calculation_Additive.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23978ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this class is implementing the additive attention given by\n",
    "## attn_weights=softmax(W_combined*tanh(W_decoder*decoder_hidden+W_Encoder*encoder_values))\n",
    "class AdditiveAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_dim, decoder_dim):\n",
    "        super(AdditiveAttention,self).__init__()\n",
    "        #256 this is the dimention of the D(t-1)\n",
    "        self.decoder_dim=decoder_dim \n",
    "        ## the diemtion of each outut of the encoder...this in our case is 256 \n",
    "        self.encoder_dim=encoder_dim\n",
    "        ## this is one of the weight matrix that will be sed to combine the weitghs\n",
    "        self.W_Combined = torch.nn.Parameter(\n",
    "            torch.FloatTensor(self.decoder_dim).uniform_(-0.1, 0.1)) \n",
    "        #FC to add a weight matrix W_decoder for the Decoder_Hidden\n",
    "        self.W_Decoder = torch.nn.Linear(decoder_dim, decoder_dim)\n",
    "        #FC to add a weight matrix to the encoder_values\n",
    "        self.W_EncoderOutput = torch.nn.Linear(encoder_dim, decoder_dim)\n",
    "\n",
    "    ##query and values is a generic terms used in case of attention \n",
    "    def get_weights(self,        \n",
    "        query: torch.Tensor,  # query = decoder hidden\n",
    "        values: torch.Tensor,  # encoder output of the form maxLength*encoder_dim in ou r case it is 10x256\n",
    "    ):\n",
    "        attn_weights = self.W_Decoder(query) + self.W_EncoderOutput(values)  # [max_length, decoder_dim]\n",
    "        return F.softmax((torch.tanh(attn_weights) @ self.W_Combined),dim=1) # [1,max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f58ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is another way of implementing the attention. In this case the attention is given by \n",
    "## attn_weight=softmax(decoder_hidden*W*encoder_values)\n",
    "\n",
    "class MultiplicativeAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_dim: int, decoder_dim: int):\n",
    "        super(MultiplicativeAttention,self).__init__()\n",
    "        ##256\n",
    "        self.decoder_dim=decoder_dim\n",
    "        ##dimention of each encoder output 256\n",
    "        self.encoder_dim=encoder_dim\n",
    "        ## weight vector of size 256\n",
    "        self.W = torch.nn.Parameter(torch.FloatTensor(\n",
    "            self.decoder_dim, self.encoder_dim).uniform_(-0.1, 0.1))\n",
    "\n",
    "    def get_weights(self,\n",
    "        query: torch.Tensor,  # [decoder_dim]\n",
    "        values: torch.Tensor, # [seq_length, encoder_dim]\n",
    "    ):\n",
    "        weights = query @ self.W @ values.T  # [1,max_length]\n",
    "        return F.softmax((weights/np.sqrt(self.decoder_dim)),dim=1)  # [1,max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92423e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is what has been used in the code https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "## basically the attention is given by  softmax(W*(Decoder_input_attime_t + Decoder_hidden_attime_t-1))\n",
    "class PytorchAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, max_length, decoder_dim):\n",
    "        super(PytorchAttention,self).__init__()\n",
    "        #256 this is the dimention of the D(t-1)\n",
    "        self.decoder_dim=decoder_dim \n",
    "        # the  maximul legth of the sequence..in our case it is 10\n",
    "        self.max_length=max_length \n",
    "        ## this will add a Weight\n",
    "        self.attn = nn.Linear(self.decoder_dim * 2, self.max_length)\n",
    "       \n",
    "    def get_weights(self,        \n",
    "        \n",
    "        hidden: torch.Tensor,  #decoder hidden at time t-1                  \n",
    "        embedded: torch.Tensor #decoder input at time t\n",
    "    ):\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded, hidden), 1)), dim=1)  # [1,seq_length]\n",
    "        return attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9daec42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size,attentionType=\"Pytorch\",dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "#         self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        ## this is different from the original code\n",
    "        self.attentionType=attentionType\n",
    "        if(self.attentionType==\"Multiplicative\"):\n",
    "            self.attention = MultiplicativeAttention(self.hidden_size,self.hidden_size)\n",
    "        elif(self.attentionType==\"Additive\"):\n",
    "            self.attention = AdditiveAttention(self.hidden_size,self.hidden_size)\n",
    "        elif(self.attentionType==\"Pytorch\"):\n",
    "            self.attention = PytorchAttention(self.max_length,self.hidden_size)\n",
    "        \n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        if(self.attentionType==\"Multiplicative\"):\n",
    "            attn_weights=self.attention.get_weights(hidden[0],encoder_outputs)\n",
    "        elif(self.attentionType==\"Additive\"):\n",
    "            attn_weights=self.attention.get_weights(hidden[0],encoder_outputs.unsqueeze(0))\n",
    "        elif(self.attentionType==\"Pytorch\"):\n",
    "            attn_weights = self.attention.get_weights(hidden[0],embedded[0])\n",
    "        \n",
    "\n",
    "#         attn_weights = F.softmax(\n",
    "#             self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "       \n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "509eba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e4c451",
   "metadata": {},
   "source": [
    "#### we are making use of the Teacher Forcing which is a way of sending the actual ground truth while the trainign is going on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "93779c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b77cd04",
   "metadata": {},
   "source": [
    "#### utility function for capturing the time of  execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "28e8ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c607a3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "309480e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cf08d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb0db13",
   "metadata": {},
   "source": [
    "### function to fetch the random samples and evaluate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3a323098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31819e5",
   "metadata": {},
   "source": [
    "### the actual training starts here with 75000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "70884b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 37s (- 8m 46s) (5000 6%) 2.6824\n",
      "1m 12s (- 7m 48s) (10000 13%) 2.1749\n",
      "1m 47s (- 7m 9s) (15000 20%) 1.6632\n",
      "2m 23s (- 6m 35s) (20000 26%) 1.1167\n",
      "3m 0s (- 6m 1s) (25000 33%) 0.7745\n",
      "3m 38s (- 5m 27s) (30000 40%) 0.5946\n",
      "4m 15s (- 4m 52s) (35000 46%) 0.4416\n",
      "4m 53s (- 4m 16s) (40000 53%) 0.3749\n",
      "5m 30s (- 3m 40s) (45000 60%) 0.3174\n",
      "6m 8s (- 3m 4s) (50000 66%) 0.3077\n",
      "6m 45s (- 2m 27s) (55000 73%) 0.2857\n",
      "7m 22s (- 1m 50s) (60000 80%) 0.2877\n",
      "7m 59s (- 1m 13s) (65000 86%) 0.2783\n",
      "8m 37s (- 0m 36s) (70000 93%) 0.2628\n",
      "9m 14s (- 0m 0s) (75000 100%) 0.2640\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words,attentionType=\"Pytorch\",dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "864e3aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> what airports does kuala lumpur have ?\n",
      "= kuala lumpur international airport and subang international airport\n",
      "< kuala lumpur international airport and subang international airport <EOS>\n",
      "\n",
      "> when was the first factory opened ?\n",
      "= 1836\n",
      "< 1836 <EOS>\n",
      "\n",
      "> how is the climate in the city ?\n",
      "= jakarta has a hot and humid equatorial/tropical climate\n",
      "< the city is hot and humid . <EOS>\n",
      "\n",
      "> were ancient mallets made of copper ?\n",
      "= no\n",
      "< no <EOS>\n",
      "\n",
      "> did faraday marry sarah barnard ?\n",
      "= yes\n",
      "< yes, faraday married sarah barnard . <EOS>\n",
      "\n",
      "> what is the biggest city in finland ?\n",
      "= the cities of the greater helsinki metropolitan area\n",
      "< the cities of the greater helsinki metropolitan area <EOS>\n",
      "\n",
      "> what happened in 1838 ?\n",
      "= don jos� ruiz y blasco was born\n",
      "< don jos� ruiz y blasco was born <EOS>\n",
      "\n",
      "> did avogadro live in england ?\n",
      "= no\n",
      "< no <EOS>\n",
      "\n",
      "> regarding this topic, what did antonio stradivari do ?\n",
      "= antonio stradivari made violins .\n",
      "< antonio stradivari made violins . <EOS>\n",
      "\n",
      "> is an official language of canada german ?\n",
      "= no, it is not .\n",
      "< no . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##check the results\n",
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aed70c",
   "metadata": {},
   "source": [
    "### code to visually see the attention while performing the translations ..in our case question should match to the right answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4e51f83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = what airports does kuala lumpur have ?\n",
      "output = kuala lumpur international airport and subang international airport <EOS>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauravp/python36/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/gauravp/python36/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = what happened in 1838 ?\n",
      "output = don jos� ruiz y blasco was born <EOS>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n",
    "evaluateAndShowAttention(\"what airports does kuala lumpur have ?\")\n",
    "\n",
    "evaluateAndShowAttention(\"what happened in 1838 ?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0b5355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
